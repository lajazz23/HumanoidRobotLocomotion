#  Other Information
 
Reinforcement learning is used to train robots on movent, and excels in locomotion-based tasks.


## Vision-Language-Action Models

Vision-Language-Action (VLA) models are a class of multimodal systems that take natural language instructions and visual input (such as images or video) and produce actions in response. These actions can range from controlling a robot arm to navigating an environment or interacting with a GUI. The goal of VLA models is to tightly integrate perception, language understanding, and decision-making, allowing for more intuitive and generalized human-computer interaction.

### GR00T-N1.5

GR00T-N1.5 is NVIDIA's newest VLA that focuses on robotics control using natural language commands grounded in vision. The model builds on the original GR00T architecture by increasing the scale, precision, and generalization capability of the model, particularly for embodied agents operating in both simulated and real-world environments.

### OpenVLA

OpenVLA (Open Vision-Language-Action) is an open-source framework and model suite for training and benchmarking VLA systems. Unlike proprietary VLA models, OpenVLA emphasizes transparency, reproducibility, and community contribution. It aims to democratize access to high-quality VLA models and accelerate research into grounded language understanding and action planning.